{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network implementation with Matrices #1: Predefined Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we implemented a neural network with 2 input layers, 3 hidden layers, and 2 output layers. To compute the loss/cost for of this network, we used the quadratic cost function. The naming style for the weights, biases and activations, and the programming style we were using made it easy to understand what is going on in this relatively small network. Programming neural networks like this, however, is computationally very inefficient, and the usage of variables in a larger network would inevitably lead to the explosion of their number. This would in fact make the code long, inefficient and unreadable. The solution to this problem is to implement neural networks by means of __matrices__ and __matrix operations__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the network we will implement by using matrices will remain the same as in the previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_26.png\" alt=\"drawing\" width=\"950\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the same dataset we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[ 1.2, 0.7],\n",
    "                 [-0.3,-0.5],\n",
    "                 [ 3.0, 0.1],\n",
    "                 [-0.1,-1.0],\n",
    "                 [-0.0, 1.1],\n",
    "                 [ 2.1,-1.3],\n",
    "                 [ 3.1,-1.8],\n",
    "                 [ 1.1,-0.1],\n",
    "                 [ 1.5,-2.2],\n",
    "                 [ 4.0,-1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([  1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                     1,\n",
    "                    -1,\n",
    "                    -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 0.7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming the elements of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the standard visualisation of the neural network architecture, abstracting the inner working of circuits by displaying only the neurons and their connections. Our network consists of the 3 layers. The input layer has 2 neurons, that used to be represented with $\\mathbf{X}$ and $\\mathbf{Y}$. The hidden layer consists of 3 neurons, and the output layer of 2 neurons. The cost function is not graphically represented here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_27.png\" alt=\"drawing\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To name the neurons, our convention will be to mark with the superscript the layer to which the neuron belongs. For example $l_1$ marks neurons belonging to the first layer. With the subscript we mark the number of a neuron within the layer. With this in mind, the third neuron of the second layer can be symbolised as $neuron_3^{(l_2)}$ or shortly $n_3^{(l_2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_28.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To name the weights the convention is to use superscript to mark the layer to which the weight belongs. For example $w^{(l_2)}$ represents __all__ the weights of the second layer. To be more specific we use the subscript that contains two numbers. The first number of the subscript defines the number of the neuron in the current layer. For example $w_1^{(l_2)}$ represents all the weights of the first neuron in the second layer. The second number of the subscript defines which activation in the previous layer does the weight parametrise (multiply).For example $w_{21}^{(l_2)}$ or perhaps better $w_{2,1}^{(l_2)}$ would indicate the weight belonging to the second neuron of the second layer that parametrises the first activation (output of the first neuron) of the previous layer $(l_1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_29.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights of the layer $l_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer $l_1$ is an input layer, containing our data points (we used to mark them as $\\mathbf{X}$ and $\\mathbf{Y}$). Since our data points are constants in the computation and the neural network uses them to learn the weights and biases (variables), its values must not change. For this reason, this layer is not parametrised by any weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights of the layer $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the layer 2 can be represented with a matrix whose number of rows matches the number of neurons in the layer 2, and whose number of colums matches the number of neurons (their activations more precisely) in the previous layer. Since the layer 2 has 3 neurons, and the layer 1 has 2 neurons, this matrix will be of the size $3x2$. The first row of the matrix represents the first neuron and its weights. The second row represents the second neuron and the third row—third neuron and its weights. The first column of the matrix represents weights that parametrise the activations of the first neuron of the previous layer. The second column of the matrix represents weights that parametrise the activations of the second neuron of the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w^{(l_2)}\n",
    "=\n",
    "\\underset{\\mathbf{3x2}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_2)} & w_{12}^{(l_2)}\\\\\n",
    "w_{21}^{(l_2)} & w_{22}^{(l_2)}\\\\\n",
    "w_{31}^{(l_2)} & w_{32}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using  _nympy_ we can easily encode the matrix $w^{(l_2)}$ and store it in a variable `w_2`. By using the function `randn`, we will fill it with the normally distributed random values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.124,  0.871],\n",
       "       [ 0.692, -0.036],\n",
       "       [ 0.455,  1.239]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2 = np.random.randn(3,2)\n",
    "w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights of the layer $l_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer 3 has 2 neurons and its previous layer ($l_2$) has 3 layers, thus 3 activations. For this reason, the weight matrix $w^{(l_3)}$ will be of the size $2x3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\\n",
    "w^{(l_3)}\n",
    "=\n",
    "\\underset{\\mathbf{2x3}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_3)} & w_{12}^{(l_3)} & w_{13}^{(l_3)}\\\\\n",
    "w_{21}^{(l_3)} & w_{22}^{(l_3)} & w_{23}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can easily store this in a variable `w_3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.006,  0.295,  0.207],\n",
       "       [ 0.126,  0.399, -0.637]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_3 = np.random.randn(2,3)\n",
    "w_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_30.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases of the layer $l_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reason the layer 1 is not parametrised with the weights, it does not also involve any biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases of the layer $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biases of the layer 2 can be represented with a single column matrix (also known as column vector) whose number of rows matches the number of neurons in the layer 2. Since the layer 2 has 3 neurons, this matrix will be of the size $3x1$. The first row of the matrix represents the bias of the first neuron, the second represents the bias of the second neuron, and the third, the the bias of the third neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b^{(l_2)}\n",
    "=\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{(l_2)}\\\\\n",
    "b_{2}^{(l_2)}\\\\\n",
    "b_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.923],\n",
       "       [ 0.02 ],\n",
       "       [-2.918]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_2 = np.random.randn(3,1)\n",
    "b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biases of the layer $l_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the third layer has two neurons, it will also have 2 biases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "b^{(l_3)}\n",
    "=\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{(l_3)}\\\\\n",
    "b_{2}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07 ],\n",
       "       [ 1.912]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_3 = np.random.randn(2,1)\n",
    "b_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_31.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations of the layer $l_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer (input layer) contains the data points. Since the data points do not change (but are in fact used to train the network by adjusting the weights and biases), this layer can be considered as an activation layer, __activating__ the neurons of the next layer. Since our data is two dimensional (each data point holds 2 numbers) the layer will have 2 neurons, each contatining a single number. Here, $a_1^{(l_1)}$ would correspond to $\\mathbf{X}$ of our previous example, and  $a_2^{(l_1)}$ would correspond to $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(l_1)}\n",
    "=\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_1)}\\\\\n",
    "a_{2}^{(l_1)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a proper neural network actual implementation (which we implement do in the next class), the cost/loss will be calculated by averaging the cost for all the data points, and well as the partial derivatives. Here, for the sake of simplicity we will compute only a single epoch of training a neural network by using a single data point. We will be using the first training example `[1.2, 0.7]` associated with the label `1`, and store its value in the variable `a_1_single`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2],\n",
       "       [0.7]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1_single = data[0].reshape(1,2).T\n",
    "a_1_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations of the layer $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the activations of the layer 2 we used to multiply each component of the data point with a corresponding weight, add the bias, and then compute the sigmoid activation: <br>`\n",
    "p1 = A1*X + B1*Y + C1 # intermediate value for the 1st neuron\n",
    "p2 = A2*X + B2*Y + C2 # intermediate value for the 2nd neuron\n",
    "p3 = A3*X + B3*Y + C3 # intermediate value for the 3rd neuron\n",
    "N1 = sigmoid(p1) # 1st neuron activation\n",
    "N2 = sigmoid(p2) # 2nd neuron activation\n",
    "N3 = sigmoid(p3) # 3rd neuron activation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by using matrix multiplication! To compute the values that would correspond to `p1`, `p2` and `p3`, called __the weighted average__, we simply need to compute a matrix product of the weight matrix of the level 2 with the activations of the level 1. To this product, we simply add the biases matrix of the level 2. To store these intermediate values, we will use the variable $z$, whose superscript will denote the layer to which this value belongs, and the subscript the neuron to which this value belongs. Thus, $z^{(l_2)}$ will indicate the weighted averages of the level 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{(l_2)}=w^{(l_2)}\\times a^{(l_1)}+b^{(l_2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}  \n",
    "z^{(l_2)}\n",
    "&=\n",
    "\\underset{\\mathbf{3x2}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_2)} & w_{12}^{(l_2)}\\\\\n",
    "w_{21}^{(l_2)} & w_{22}^{(l_2)}\\\\\n",
    "w_{31}^{(l_2)} & w_{32}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "\\times\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_1)}\\\\\n",
    "a_{2}^{(l_1)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "+\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{(l_2)}\\\\\n",
    "b_{2}^{(l_2)}\\\\\n",
    "b_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\\\\\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "(w_{11}^{(l_2)} * a_{1}^{(l_1)}) + (w_{12}^{(l_2)}*a_{2}^{(l_1)})+b_{1}^{(l_2)}\\\\\n",
    "(w_{21}^{(l_2)} * a_{1}^{(l_1)}) + (w_{22}^{(l_2)}*a_{2}^{(l_1)})+b_{2}^{(l_2)}\\\\\n",
    "(w_{31}^{(l_2)} * a_{1}^{(l_1)}) + (w_{32}^{(l_2)}*a_{2}^{(l_1)})+b_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\\\\\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "z_1^{(l_2)}\\\\\n",
    "z_2^{(l_2)}\\\\\n",
    "z_3^{(l_2)}\\\\\n",
    "\\end{bmatrix}}\n",
    "\\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be simply computed by using the numpy function `dot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.462],\n",
       "       [ 0.825],\n",
       "       [-1.504]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_2 = w_2.dot(a_1_single) + b_2\n",
    "z_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to compute the activations, we just need to apply sigmoid activation to the matrix $z^{(l_2)}$ `(z_2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(l_2)}\n",
    "\\equiv\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)}\\\\\n",
    "a_{2}^{(l_2)}\\\\\n",
    "a_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(z^{(l_2)})\n",
    "\\equiv\n",
    "\\sigma\n",
    "\\left(\\;\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{(l_2)}\\\\\n",
    "z_{2}^{(l_2)}\\\\\n",
    "z_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\;\n",
    "\\right)\n",
    "\\equiv\n",
    "\\begin{bmatrix}\n",
    "\\sigma(z_1^{(l_2)})\\\\\n",
    "\\sigma(z_2^{(l_2)})\\\\\n",
    "\\sigma(z_3^{(l_2)})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the activations of the layer $l_2$,  $a^{(l_2)}$ and store it in the variable `a_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.386],\n",
       "       [0.695],\n",
       "       [0.182]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2 = sigmoid(z_2)\n",
    "a_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations of the layer $l_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the activations for the layer 3, we apply the same procedure we did for the level 2. To compute the weighted average matrix of the layer 3 we compute a matrix product of the weight matrix of the layer 3 with the activation matrix of the layer before (layer 2), and add the bias matrix of the level 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z^{(l_3)}=w^{(l_3)}\\times a^{(l_2)}+b^{(l_3)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}  \n",
    "z^{(l_3)}\n",
    "&=\n",
    "\\underset{\\mathbf{2x3}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_3)} & w_{12}^{(l_3)} & w_{13}^{(l_3)}\\\\\n",
    "w_{21}^{(l_3)} & w_{22}^{(l_3)} & w_{23}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "\\times\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)}\\\\\n",
    "a_{2}^{(l_2)}\\\\\n",
    "a_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "+\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{(l_3)}\\\\\n",
    "b_{2}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\\\\\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "(w_{11}^{(l_3)} * a_{1}^{(l_2)}) + (w_{12}^{(l_3)}*a_{2}^{(l_2)})+ (w_{13}^{(l_3)}*a_{3}^{(l_2)})+b_{1}^{(l_3)}\\\\\n",
    "(w_{21}^{(l_3)} * a_{1}^{(l_2)}) + (w_{22}^{(l_3)}*a_{2}^{(l_2)})+ (w_{22}^{(l_3)}*a_{2}^{(l_2)})+b_{2}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "z_1^{(l_3)}\\\\\n",
    "z_2^{(l_3)}\\\\\n",
    "\\end{bmatrix}}\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17 ],\n",
       "       [2.122]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_3 = w_3.dot(a_2)+b_3\n",
    "z_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to compute the final activation of the layer 3 (output layer), we apply the sigmoid function to the weighted average:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(l_3)}\n",
    "\\equiv\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_3)}\\\\\n",
    "a_{2}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(z^{(l_3)})\n",
    "\\equiv\n",
    "\\begin{bmatrix}\n",
    "\\sigma(z_1^{(l_3)})\\\\\n",
    "\\sigma(z_2^{(l_3)})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.542],\n",
       "       [0.893]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_3 = sigmoid(z_3)\n",
    "a_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the backward pass:  Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weighted averages and activations for each layer, computation of the forward pass is complete. The next step is to compute the network cost of our training example `[1.2, 0.7]`, and then the backward pass by computing the partial derivatives of this cost in respect to all the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The data point we were using to compute the forwared pass contains the values `[1.2, 0.7]` , and it is associated with the label `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous case, we need to interpret the data labels `-1` and `1` by means of 2 neurons. The output of the first neuron encodes the probability that the label is `-1`, and the second the probability that the label is `1`. To do this, we can reuse the function `convert_label` from the last lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if (label == -1):\n",
    "        return (1,0)\n",
    "    if (label == 1):\n",
    "        return (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_label(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_label(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we convert the label `1` associated with the data point `[1.2, 0.7]` into the variable `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(convert_label(labels[0])).reshape(2,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be using the _quadratic cost function_ to compute the cost of our example. Since we have two output layers, the formula for computing the cost of a single training example we will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_x = \\frac{1}{2}\n",
    "\\sum_{k=1}^2 (y_k-a_k^{(l_3)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we need to substract the activation of each neuron in the output layer from the true label associated with this neuron, square it and sum the squared terms. The reason we are multiplying the sum by $\\frac{1}{2}$ ist that it will simplify the computation of the derivative. This same equation can be represented in simpler terms by using vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C_x =\n",
    "\\frac{1}{2} \\|y-a^{(l_3)} \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the result of this computation in the variable `C_x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15280874657485913"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_x = 0.5*sum((a_3 - y)**2)[0]\n",
    "C_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of multiplying the cost function by $\\frac{1}{2}$, the computation of the partial derivatives in $\\frac{\\partial C_x}{\\partial a^{(l_3)}}$ boils down to a difference between two column vectors:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a^{(l_3)}} = (a^{(l_3)}-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.542],\n",
       "       [-0.107]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a_3 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute all the weights and biases we will be using a strategy that can be well illustrated on the diagram of the previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/neural_networks_26.png\" alt=\"drawing\" width=\"950\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computation `(a_3-y)` we have computed the derivatives of the cost of a single data point in respect to the activations of the layer 3 (output layer), designated in the last example by the variables $0_1$ and $0_2$.<br><br>\n",
    "Efficiently computing the values of partial derivatives with matrices requires a strategy. The idea is to first compute partial derivatives of the cost/loss in respect to the __weighted sums__—intermediate values in each layer of the network (excluding the first layer which contains only our data points). Knowing these values allow us to easily compute the partial derivatives of all the weights and biases.<br>\n",
    "In our previous example, the intermediate values of the third (output) layer were $z_1$ and $z_2$ and the intermediate values of the second layer $p_1$, $p_2$ and $p_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using matrices, we will store these values accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta^{(l_3)}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_3)}\\\\\n",
    "\\delta_2^{(l_3)}\\\\\n",
    "\\end{bmatrix};\\quad\n",
    "\\delta^{(l_2)}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_2)}\\\\\n",
    "\\delta_2^{(l_2)}\\\\\n",
    "\\delta_3^{(l_2)}\\\\\n",
    "\\end{bmatrix};\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the  weighted sums in the output layer $\\delta^{(l_3)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the $\\delta^{(l_3)}$, we need to use the chain rule and multiply the partial derivatives $\\frac{\\partial C}{\\partial a^{(l_3)}}$ which we computed as `(a_3-y)`, and the partial derivative $\\frac{\\partial a^{(l_3)}}{\\partial z^{(l_3)}}$, which is simply a derivative of a sigmoid function $\\sigma^{\\prime}$ applied to each element of a vector:\n",
    "$$\n",
    "\\sigma^{\\prime}\n",
    "=\n",
    "\\sigma(z) * (1-\\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the $\\frac{\\partial a^{(l_3)}}{\\partial z^{(l_3)}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.248],\n",
       "       [0.096]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(z_3) * (1 - sigmoid(z_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have already computed `a_3 = sigmoid(z_3)` in the forward pass, we can be efficient, and reuse this computation to compute the derivative of the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.248],\n",
       "       [0.096]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_3 * (1 - a_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore computing the $\\delta^{(l_3)}$ only requires us to use the chain rule:<br>\n",
    "$$\n",
    "\\delta^{(l_3)}\n",
    "\\equiv\n",
    "\\frac{\\partial C}{\\partial \\delta^{(l_3)}} = \\frac{\\partial C}{\\partial a^{(l_3)}} * \\frac{\\partial a^{(l_3)}}{z^{(l_3)}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.135],\n",
       "       [-0.01 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_3 = (a_3 - y) * a_3 * (1 - a_3)\n",
    "delta_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the weighted sums in the hidden layer $\\delta^{(l_2)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hadamard product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer science, we often need multiply two vectors, or matrices elementwise. This simple operation is not so well known in mathematics, so we will introduce it here. If $p$ and $t$ are two vectors of the same dimension, we can use $p⊙t$ to denote the elementwise product of the two vectors, called a __Hadamard product__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} a \\\\ b \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} c \\\\ d\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} a \\cdot c \\\\ b \\cdot d \\end{array} \\right]\n",
    "\\tag{28}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient computation of the $\\delta^{(l_2)}$ should let us use the already computed partial derivatives in the matrix  $\\delta^{(l_3)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example, to compute the partial derivatives of the activations of the second layer `dN1`, `dN2` and `dN3`, we needed to take into consideration that the computation of the partial derivative of `dN1` involves considering two paths, one involving `dz1` and another `dz2`, and summing their outputs.\n",
    "\n",
    "`dN1 = dz1*A4 + dz2*A5\n",
    "dN2 = dz1*B4 + dz2*B5\n",
    "dN3 = dz1*C4 + dz2*C5`\n",
    "\n",
    "After that, to compute the partial derivatives of the sigmoid activations in respect to the __weighted averages__ `dp1`, `dp2`, and `dp3`, we multiplied the activations with the partial derivatives of the sigmoid activation in respect to the weighted sums:<br>\n",
    "\n",
    "`dp1 = dN1*N1*(1-N1)\n",
    "dp2 = dN2*N2*(1-N2)\n",
    "dp3 = dN3*N3*(1-N3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same procedure can be implemented with matrices by using the __transpose__ of the weight matrix. For example the transpose of the matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\\n",
    "w^{(l_3)}\n",
    "=\n",
    "\\underset{\\mathbf{2x3}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_3)} & w_{12}^{(l_3)} & w_{13}^{(l_3)}\\\\\n",
    "w_{21}^{(l_3)} & w_{22}^{(l_3)} & w_{23}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left(w^{(l_3)}\\right)^T\n",
    "=\n",
    "\\underset{\\mathbf{3x2}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_3)} & w_{21}^{(l_3)}\\\\\n",
    "w_{12}^{(l_3)} & w_{22}^{(l_3)}\\\\\n",
    "w_{13}^{(l_3)} & w_{23}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.006,  0.126],\n",
       "       [ 0.295,  0.399],\n",
       "       [ 0.207, -0.637]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_3.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of this column contains all the weights of the first neuron, and the second all the weights of the second neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compute a matrix product between $\\left(w^{(l_3)}\\right)^T$ and $\\delta^{(l_3)}$ we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\left(w^{(l_3)}\\right)^T\\delta^{(l_3)}\n",
    "&=\n",
    "\\underset{\\mathbf{3x2}}{\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(l_3)} & w_{21}^{(l_3)}\\\\\n",
    "w_{12}^{(l_3)} & w_{22}^{(l_3)}\\\\\n",
    "w_{13}^{(l_3)} & w_{23}^{(l_3)}\\\\\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\times\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_3)}\\\\\n",
    "\\delta_2^{(l_3)}\\\\\n",
    "\\end{bmatrix}}\\\\\n",
    "\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "(w_{11}^{(l_3)}*\\delta_1^{(l_3)})+(w_{21}^{(l_3)}*\\delta_2^{(l_3)})\\\\\n",
    "(w_{12}^{(l_3)}*\\delta_1^{(l_3)})+(w_{22}^{(l_3)}*\\delta_2^{(l_3)})\\\\\n",
    "(w_{13}^{(l_3)}*\\delta_1^{(l_3)})+(w_{23}^{(l_3)}*\\delta_2^{(l_3)})\\\\\n",
    "\\end{bmatrix}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.002],\n",
       "       [ 0.036],\n",
       "       [ 0.034]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_3.T.dot(delta_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example would be the equivalent to <br><br>\n",
    "`dN1 = dz1*A4 + dz2*A5\n",
    "dN2 = dz1*B4 + dz2*B5\n",
    "dN3 = dz1*C4 + dz2*C5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left is to multiply this matrix with the partial derivatives of the sigmoid activation  $\\sigma'(z^{(l_2)})$ which we compute in the same way as for $\\delta^{(l_3)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.237],\n",
       "       [0.212],\n",
       "       [0.149]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2 * (1 - a_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is the formula to compute the weighted averages matrix of the layer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta^{(l_2)} = ((w^{(l_3)})^T \\delta^{(l_3)}) \\odot \\sigma'(z^{(l_2)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute this we simply need to multiply `w_3.T.dot(delta_3)` and `a_2 * (1 - a_2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.   ],\n",
       "       [ 0.008],\n",
       "       [ 0.005]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_2 = np.dot(w_3.T,delta_3) * a_2 * (1 - a_2)\n",
    "delta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the biases in the output layer $b^{(l_3)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the partial derivatives of the cost in respect to the weighted averages, computing the biases is trivial. Since a bias is only added to the multiplications of weights and activations, its derivative is equal to 1. Therefore, the biases of a certain layer have the same value as the weighted averages of that layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.135],\n",
       "       [-0.01 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_3 = delta_3\n",
    "db_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the biases of the hidden layer $b^{(l_2)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.   ],\n",
       "       [ 0.008],\n",
       "       [ 0.005]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_2 = delta_2\n",
    "db_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the weights in the output layer $w^{(l_3)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example, to compute the partial derivatives of the weights of the first neuron of the output layer `dA4`, and `dA5`, we simply multipled the values of the intermediate steps `dz1`, and `dz3` with he value of the first activation `N1`. To compute the partial derivatives of the weights of the second neuron of the output layer `dB4`, and `dB5`, we simply multipled the values of the intermediate steps `dz1`, and `dz3` with he value of the second activation `N2`. Finally, to compute the partial derivatives of the weights of the third neuron of the output layer `dC4`, and `dC5`, we simply multipled the values of the intermediate steps `dz1`, and `dz3` with he value of the third activation `N3`.\n",
    "\n",
    "`dA4 = dz1*N1\n",
    "dA5 = dz2*N1`\n",
    "\n",
    "`dB4 = dz1*N2\n",
    "dB5 = dz2*N2`\n",
    "\n",
    "`dC4 = dz1*N3\n",
    "dC5 = dz2*N3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this with matrix multiplication, we need to compute the transpose matrix of the activations of the previous layer $l_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "a^{(l_2)}\n",
    "&\\equiv\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)}\\\\\n",
    "a_{2}^{(l_2)}\\\\\n",
    "a_{3}^{(l_2)}\\\\\n",
    "\\end{bmatrix}}\n",
    "\\\\\\\\\n",
    "\\left(a^{(l_2)}\\right)^T\n",
    "&\\equiv\n",
    "\\underset{\\mathbf{1x3}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)} & a_{2}^{(l_2)} & a_{3}^{(l_2)}\n",
    "\\end{bmatrix}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.386, 0.695, 0.182]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the matrix product of the already computed weighted average of the layer 3 and this transposed activation matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\delta^{(l_3)}\\times\\left(a^{(l_2)}\\right)^T\n",
    "&=\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_3)}\\\\\n",
    "\\delta_2^{(l_3)}\\\\\n",
    "\\end{bmatrix}}\n",
    "\\times\n",
    "\\underset{\\mathbf{1x3}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)} & a_{2}^{(l_2)} & a_{3}^{(l_2)}\n",
    "\\end{bmatrix}}\\\\\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{2x3}}{\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_3)} a_{1}^{(l_2)} & \\delta_1^{(l_3)} a_{2}^{(l_2)} & \\delta_1^{(l_3)} a_{3}^{(l_2)} \\\\\n",
    "\\delta_2^{(l_3)} a_{1}^{(l_2)} & \\delta_2^{(l_3)} a_{2}^{(l_2)} & \\delta_2^{(l_3)} a_{3}^{(l_2)} \\\\\n",
    "\\end{bmatrix}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.052,  0.094,  0.024],\n",
       "       [-0.004, -0.007, -0.002]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_3 = delta_3.dot(a_2.T)\n",
    "dw_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the same 6 values as in `dA4`, `dA5`, `dB4`, `dB5`, `dC4`,and  `dC5`, compactly represented in a single matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives of the weights in the hidden layer $w^{(l_2)}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example, to compute the partial derivatives of the weights of the second layer `dA1`, `dA2`, `dA3`, we simply multipled the values of the intermediate steps `dp1`, `dp2`, and `dp3` with he value of the first activation (data point) $\\mathbf{X}$. To compute the partial derivatives of the weights `dB1`, `dB2` and `dB3`, we multipled the same intermediate values `dp1`, `dp2`, and `dp3` with the value of the second activation (data point) $\\mathbf{Y}$.\n",
    "\n",
    "`dA1 = dp1*X\n",
    "dA2 = dp2*X\n",
    "dA3 = dp3*X`\n",
    "\n",
    "`dB1 = dp1*Y\n",
    "dB2 = dp2*Y\n",
    "dB3 = dp3*Y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this with matrix multiplication, we need to compute the transpose matrix of the activations of the previous (input) layer $l_1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "a^{(l_1)}\n",
    "&\\equiv\n",
    "\\underset{\\mathbf{2x1}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_1)}\\\\\n",
    "a_{2}^{(l_1)}\\\\\n",
    "\\end{bmatrix}}\\\\\n",
    "\\\\\n",
    "\\left(a^{(l_1)}\\right)^T\n",
    "&\\equiv\n",
    "\\underset{\\mathbf{1x2}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_1)} & a_{2}^{(l_1)}\n",
    "\\end{bmatrix}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2, 0.7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1_single.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the dot product of the already computed weighted average of the layer 2 and this transposed activation matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\delta^{(l_2)}\\times\\left(a^{(l_1)}\\right)^T\n",
    "&=\n",
    "\\underset{\\mathbf{3x1}}{\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_2)}\\\\\n",
    "\\delta_2^{(l_2)}\\\\\n",
    "\\delta_3^{(l_2)}\\\\\n",
    "\\end{bmatrix}}\n",
    "\\times\n",
    "\\underset{\\mathbf{1x2}}{\n",
    "\\begin{bmatrix}\n",
    "a_{1}^{(l_2)} & a_{2}^{(l_2)}\n",
    "\\end{bmatrix}}\\\\\\\\\n",
    "&=\n",
    "\\underset{\\mathbf{3x2}}{\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(l_3)} a_{1}^{(l_2)} & \\delta_1^{(l_3)} a_{2}^{(l_2)} \\\\\n",
    "\\delta_2^{(l_3)} a_{1}^{(l_2)} & \\delta_2^{(l_3)} a_{2}^{(l_2)} \\\\\n",
    "\\delta_3^{(l_3)} a_{1}^{(l_2)} & \\delta_3^{(l_3)} a_{2}^{(l_2)}\n",
    "\\end{bmatrix}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.001, -0.   ],\n",
       "       [ 0.009,  0.005],\n",
       "       [ 0.006,  0.004]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw_2 = delta_2.dot(a_1_single.T)\n",
    "dw_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the same 6 values as in `dA1`, `dA2`, `dA3`, `dB1`, `dB2`,and  `dB3` compactly represented in a single matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the weights and the biases of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to update the weights gradually (slowly descend the function's gradient).  For this reason we will use the variable `step_size` with which we multiply the weights and biases matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1\n",
    "# updating the weights and biases of the third (output) layer\n",
    "w_3 -= dw_3*step_size\n",
    "b_3 -= db_3*step_size\n",
    "# updating the weights and biases of the second (hidden) layer\n",
    "w_2 -= dw_2*step_size\n",
    "b_2 -= db_2*step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to achieve with training is that our network outputs the value as close as possible to the label, which the data is associated with. The current label is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our old activation was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.542],\n",
       "       [0.893]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the new activation by computing the forward pass again with the updated weights and biases. The new activation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.537],\n",
       "       [0.893]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2 = sigmoid(w_2.dot(a_1_single) + b_2)\n",
    "a_3 = sigmoid(w_3.dot(a_2) + b_3)\n",
    "a_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is better than the previous one, which proves that the training is working. Now, this step needs to be repeated until the appropriate approximation of the label is reached. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
